{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import winsound\n",
    "import time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of users: 200938\n"
     ]
    }
   ],
   "source": [
    "#clinton_trump_tweets.txt\n",
    "file = open(\"clinton_trump_tweets.txt\", \"r\")\n",
    "tweetCounter = {}\n",
    "usernameConvertId = {}\n",
    "idConvertUser = {}\n",
    "\n",
    "for line in file:\n",
    "    line = line.replace('\\n', \"\")\n",
    "    splittedTweet = line.split(\"\\t\")\n",
    "    for items in range(len(splittedTweet)):\n",
    "        if(splittedTweet[items].isdigit()):\n",
    "            username = splittedTweet[items - 1]\n",
    "            if(username in tweetCounter):\n",
    "                tweetCounter[username] = tweetCounter[username] + 1\n",
    "            else:\n",
    "                tweetCounter[username] = 1\n",
    "            if(not(str(splittedTweet[items]) in usernameConvertId)):\n",
    "                usernameConvertId[str(splittedTweet[items])] = username\n",
    "                idConvertUser[username] = str(splittedTweet[items])\n",
    "            break\n",
    "\n",
    "print(\"Total number of users: \" + str(len(tweetCounter)))\n",
    "file.close()\n",
    "winsound.Beep(450, 2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Afairw tous xrhstes pou exoun plhthos tweets ligotero apo 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of users that have <10 tweets: 70608\n"
     ]
    }
   ],
   "source": [
    "namesTodelete = []\n",
    "namesToKeep = []\n",
    "finalID = {}\n",
    "finalNames = {}\n",
    "\n",
    "for user in tweetCounter:\n",
    "    if(tweetCounter[user] <= 10):\n",
    "        namesTodelete.append(user)\n",
    "    else:\n",
    "        namesToKeep.append(user)\n",
    "        \n",
    "for i in namesTodelete:\n",
    "    del tweetCounter[i]\n",
    "    \n",
    "for i in set(namesToKeep):\n",
    "    finalNames[i] = idConvertUser[i]\n",
    "    finalID[idConvertUser[i]] = i  \n",
    "print(\"Total number of users that have <10 tweets: \" + str(len(tweetCounter)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RachelMM77 : 1704696793\n",
      "1704696793 : RachelMM77\n"
     ]
    }
   ],
   "source": [
    "for i in finalNames:\n",
    "    print(str(i) + \" : \" + str(finalNames[i]))\n",
    "    break\n",
    "for i in finalID:\n",
    "    print(str(i) +  \" : \" + str(finalID[i]))\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tha grapsw se ena arxeio me onoma \"tweetsToCheck.txt\" ola ta tweets pou exoun grapsei mono oi xrhstes pou exw katalhxei na exw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fileToWrite = open(\"tweetsToCheck.txt\", \"w\")\n",
    "file = open(\"clinton_trump_tweets.txt\", \"r\")\n",
    "setNames = set(list(finalNames.keys()))\n",
    "\n",
    "for line in file:\n",
    "    line1 = line\n",
    "    line = line.replace('\\n', \"\")\n",
    "    splittedTweet = line.split(\"\\t\")\n",
    "    for items in range(len(splittedTweet)):\n",
    "        if(splittedTweet[items].isdigit()):\n",
    "            username = splittedTweet[items - 1]\n",
    "            if(username in setNames):\n",
    "                fileToWrite.write(line1)\n",
    "            break\n",
    "file.close()\n",
    "fileToWrite.close()\n",
    "winsound.Beep(450, 2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apo autoys tous xrhstes tha krathse ta hashtags/handles pou exoune kanei, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getTagsHandles(x):\n",
    "    symbols = [' ', '\\n', ',', '.', '/', '//', '&', '?', ';', '[', ']', '!', ':', 'â€¢', '\\\\', ')']\n",
    "    index = 0\n",
    "    word = \"\"\n",
    "    hh = []\n",
    "    \n",
    "    while(index < len(x)):\n",
    "        if(x[index] == \"@\" or x[index] == \"#\"):\n",
    "            if(word == \"\"):\n",
    "                word = word + x[index]\n",
    "            else:\n",
    "                if(len(word) == 1):\n",
    "                    word = \"\" + x[index]\n",
    "                else:\n",
    "                    hh.append(word)\n",
    "                    word = \"\"\n",
    "                    continue\n",
    "        elif(x[index] in symbols or ord(x[index]) == 8230):\n",
    "            if(word != \"\"):\n",
    "                hh.append(word)\n",
    "                word = \"\"\n",
    "        else:\n",
    "            if(word == \"\"):\n",
    "                index = index + 1\n",
    "                continue\n",
    "            else:\n",
    "                word = word + x[index]\n",
    "\n",
    "        index = index + 1\n",
    "    if(len(word) > 1):\n",
    "        hh.append(word)\n",
    "        \n",
    "    return (hh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "file = open(\"tweetsToCheck.txt\", \"r\")\n",
    "hhDict = {}\n",
    "locationDict = {}\n",
    "testFile = open(\"test.txt\", \"w\")\n",
    "\n",
    "for line in file:\n",
    "    testFile.write(line)\n",
    "    line = line.replace(\"\\n\", \"\")\n",
    "    splittedTweet = line.split(\"\\t\")\n",
    "    for items in range(len(splittedTweet)):\n",
    "        if(splittedTweet[items].isdigit()):\n",
    "            username = splittedTweet[items - 1]\n",
    "            hhDict[username] = []\n",
    "            hhL = getTagsHandles(splittedTweet[-1])\n",
    "            if(username in hhDict):\n",
    "                hhDict[username] = hhDict[username] + hhL\n",
    "            indexLocation = items + 3\n",
    "            if(not(username in locationDict)):\n",
    "                locationDict[username] = splittedTweet[indexLocation]\n",
    "            break\n",
    "\n",
    "testFile.close()\n",
    "file.close()\n",
    "winsound.Beep(450, 2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Name, ScreenName, UserID, FollowersCount, FriendsCount, Location, Description, CreatedAt, StatusID,\n",
    "Language, Place, RetweetCount, FavoriteCount, Text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apo to arxeio pou mou leei poioi einai oi akolouthoi tou trump kai ths clinton tha kanw thn antistoixia me tis dikes mou domes wste na dw poios akolouthei poion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of Trump followers: 48.45909811919329%\n",
      "['jj_4884', 'vigilant_Amey', 'kaceyjoyner', 'elise_vendrone', 'robertncheek']\n",
      "Total number of Clinton followers: 51.52390663947428%\n",
      "['lateshakharizma', 'AvecAnneHidalgo', 'BluntBong', 'craigrettig', 'seeritarun']\n"
     ]
    }
   ],
   "source": [
    "file = open(\"clinton_trump_user_classes.txt\", \"r\")\n",
    "trumpFollowers = []\n",
    "clintonFollowers = []\n",
    "trumpFollowersCounter = 0\n",
    "clintonFollowersCounter = 0\n",
    "setID = set(list(finalID.keys()))\n",
    "\n",
    "for line in file:\n",
    "    line = line.replace(\"\\n\", \"\")\n",
    "    text = line.split(\"\\t\")\n",
    "    idV = str(text[0])\n",
    "    followV = int(text[1])\n",
    "    if(idV in setID):\n",
    "        if(followV == 0):\n",
    "            trumpFollowersCounter = trumpFollowersCounter + 1\n",
    "            trumpFollowers.append(finalID[idV])\n",
    "        elif(followV == 1):\n",
    "            clintonFollowers.append(finalID[idV])\n",
    "            clintonFollowersCounter = clintonFollowersCounter + 1\n",
    "        \n",
    "print(\"Total number of Trump followers: \" + str(trumpFollowersCounter*100/len(finalNames)) + \"%\")\n",
    "print(trumpFollowers[0:5])\n",
    "print(\"Total number of Clinton followers: \" + str(clintonFollowersCounter*100/len(finalNames)) + \"%\")\n",
    "print(clintonFollowers[0:5])\n",
    "file.close()\n",
    "winsound.Beep(450, 2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy.sparse as sp_sparse\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sklearn as sk\n",
    "import sklearn.datasets as sk_data\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn import preprocessing\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "import sklearn.utils as utils\n",
    "from sklearn import tree\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import svm\n",
    "import sklearn.linear_model as linear_model\n",
    "import sklearn.cross_validation as cross_validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of assignments: 100508\n",
      "Total number of assignments: 91816\n",
      "[('@FanDuel', '40 George Street Edinburgh', 1), ('@TheCut', 'Los Angeles', 0), ('#florida', 'Celebration, FL', 0), ('@MLBONFOX', 'Jersey / Philly', 0), ('@AoDespair', 'NYC', 1)]\n"
     ]
    }
   ],
   "source": [
    "ekxwrhseis = []\n",
    "tf = set(trumpFollowers)\n",
    "counter = 0\n",
    "\n",
    "for i in hhDict:\n",
    "    name = i\n",
    "    location = locationDict[name]\n",
    "    hh = hhDict[name]\n",
    "    if(name in tf):\n",
    "        follow = 0\n",
    "    else:\n",
    "        follow = 1\n",
    "    for j in hh:\n",
    "        ekxwrhseis.append([])\n",
    "        ekxwrhseis[counter].append(j)\n",
    "        ekxwrhseis[counter].append(location)\n",
    "        ekxwrhseis[counter].append(follow)\n",
    "        counter = counter + 1\n",
    "    \n",
    "print(\"Total number of assignments: \" + str(len(ekxwrhseis)))\n",
    "ekxwrhseisSet = set([])\n",
    "for i in ekxwrhseis:\n",
    "    ekxwrhseisSet.add(tuple(i))\n",
    "ekxwrhseisSet = list(ekxwrhseisSet)\n",
    "print(\"Total number of assignments: \" + str(len(ekxwrhseisSet)))\n",
    "print(ekxwrhseisSet[0:5])\n",
    "winsound.Beep(450, 2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import binascii\n",
    "\n",
    "def hashString(s):\n",
    "    b = s.encode('ascii')\n",
    "    h = binascii.crc32(b) & 0xffffffff\n",
    "    return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('@FanDuel', '40 George Street Edinburgh', 1), ('@TheCut', 'Los Angeles', 0), ('#florida', 'Celebration, FL', 0), ('@MLBONFOX', 'Jersey / Philly', 0), ('@AoDespair', 'NYC', 1), ('#PartsUnknown', '', 1), ('@megynkelly', 'Ohio', 1), ('#AFM2016', '', 1), ('#Transgender', 'New York, NY', 1), ('@TimAlberta', '', 1)]\n",
      "91816\n",
      "[[1172266414.0, 363337892.0, 1], [4103855088.0, 363337892.0, 0], [2890591945.0, 4062261174.0, 0], [3517917143.0, 3789792396.0, 0], [1480205588.0, 3789792396.0, 1], [1412416945.0, 3789792396.0, 1], [50458423.0, 3789792396.0, 1], [1910281131.0, 3789792396.0, 1], [1948257494.0, 3508117254.0, 1], [3163033379.0, 3508117254.0, 1]]\n"
     ]
    }
   ],
   "source": [
    "print(ekxwrhseisSet[0:10])\n",
    "convertEkxwrhseis = []\n",
    "for i in ekxwrhseisSet:\n",
    "    if(\"@\" in i[0]):\n",
    "        h = i[0].replace(\"@\", \"\")\n",
    "    else:\n",
    "        h = i[0].replace(\"#\", \"\")\n",
    "    s = hashString(h)\n",
    "    if(\",\" in i[1]):\n",
    "        location = i[1].replace(\",\", \"\")\n",
    "    if(\"/\" in i[1]):\n",
    "        location = i[1].replace(\"/\", \"\")\n",
    "    l = hashString(location)\n",
    "    newList = [float(s), float(l), i[2]]\n",
    "    convertEkxwrhseis.append(newList)\n",
    "    \n",
    "print(len(convertEkxwrhseis))\n",
    "print(convertEkxwrhseis[0:10])\n",
    "winsound.Beep(450, 2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 -- 18363\n",
      "18363 -- 36726\n",
      "36726 -- 55089\n",
      "55089 -- 73452\n",
      "73452 -- 91816\n"
     ]
    }
   ],
   "source": [
    "sumC = 0\n",
    "splitD = int(len(convertEkxwrhseis)/5)\n",
    "x = []\n",
    "for i in range(5):\n",
    "    if(i == 4):\n",
    "        end = len(convertEkxwrhseis)\n",
    "    else:\n",
    "        end = sumC + splitD\n",
    "    print(str(sumC) + \" -- \" + str(end))\n",
    "    sumC = sumC + splitD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_set_size = 45000\n",
    "\n",
    "X_train = []\n",
    "for i in range(train_set_size):\n",
    "    newElement = [convertEkxwrhseis[i][0], convertEkxwrhseis[i][1]]\n",
    "    X_train.append(newElement)\n",
    "y_train = []\n",
    "for i in range(train_set_size):\n",
    "    y_train.append(convertEkxwrhseis[i][2])\n",
    "    \n",
    "    \n",
    "X_test = []\n",
    "for i in range(train_set_size, len(convertEkxwrhseis)):\n",
    "    newElement = [convertEkxwrhseis[i][0], convertEkxwrhseis[i][1]]\n",
    "    X_test.append(newElement)\n",
    "y_test = []\n",
    "for i in range(train_set_size,len(convertEkxwrhseis)):\n",
    "    y_test.append(convertEkxwrhseis[i][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def statistics(y_pred, y_test):\n",
    "    print(metrics.accuracy_score(y_test,y_pred))\n",
    "    print(metrics.confusion_matrix(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TIME: 1.3633602322248066 seconds\n",
      "0.519800922761\n",
      "[[10321 11333]\n",
      " [11148 14014]]\n"
     ]
    }
   ],
   "source": [
    "start = time.clock()\n",
    "dtree = tree.DecisionTreeClassifier()\n",
    "dtree = dtree.fit(X_train, y_train)\n",
    "y_pred = dtree.predict(X_test)\n",
    "print(\"TIME: \" + str(time.clock() - start) + \" seconds\")\n",
    "statistics(y_pred, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18364 18365 18366 ..., 91813 91814 91815] [    0     1     2 ..., 18361 18362 18363]\n",
      "[    0     1     2 ..., 91813 91814 91815] [18364 18365 18366 ..., 36724 36725 36726]\n",
      "[    0     1     2 ..., 91813 91814 91815] [36727 36728 36729 ..., 55087 55088 55089]\n",
      "[    0     1     2 ..., 91813 91814 91815] [55090 55091 55092 ..., 73450 73451 73452]\n",
      "[    0     1     2 ..., 73450 73451 73452] [73453 73454 73455 ..., 91813 91814 91815]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "only integer arrays with one element can be converted to an index",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-39-b558550ba906>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mkf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconvertEkxwrhseis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"%s %s\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconvertEkxwrhseis\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconvertEkxwrhseis\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconvertEkxwrhseis\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconvertEkxwrhseis\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: only integer arrays with one element can be converted to an index"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "kf = KFold(n_splits=5)\n",
    "for train, test in kf.split(convertEkxwrhseis):\n",
    "    print(\"%s %s\" % (train, test))\n",
    "#X_train, X_test, y_train, y_test = convertEkxwrhseis[train], convertEkxwrhseis[test], convertEkxwrhseis[train], convertEkxwrhseis[test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "start = time.clock()\n",
    "svm_clf = svm.SVC()\n",
    "svm_clf.fit(X_train,y_train)\n",
    "#print(svm_clf.score(X_test,y_test))\n",
    "y_pred = svm_clf.predict(X_test)\n",
    "print(\"TIME: \" + str(time.clock() - start) + \" seconds\")\n",
    "#print(metrics.confusion_matrix(y_test,y_pred))\n",
    "statistics(y_pred, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#import sklearn.linear_model as linear_model\n",
    "start = time.clock()\n",
    "lr_clf = linear_model.LogisticRegression()\n",
    "lr_clf.fit(X_train, y_train)\n",
    "#print(lr_clf.score(X_test,y_test))\n",
    "y_pred = lr_clf.predict(X_test)\n",
    "print(\"TIME: \" + str(time.clock() - start) + \" seconds\")\n",
    "statistics(y_pred, y_test)\n",
    "#print(metrics.confusion_matrix(y_test,y_pred))\n",
    "#probs = lr_clf.predict_proba(X_test)\n",
    "#print (probs)\n",
    "#print (probs.argmax(axis = 1))\n",
    "#print (probs.max(axis = 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#from sklearn.neighbors import KNeighborsClassifier\n",
    "start = time.clock()\n",
    "knn = KNeighborsClassifier(n_neighbors=3)\n",
    "knn.fit(X_train,y_train)\n",
    "#print(knn.score(X_test,y_test))\n",
    "\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"TIME: \" + str(time.clock() - start) + \" seconds\")\n",
    "#print(metrics.confusion_matrix(y_test,y_pred))\n",
    "statistics(y_pred, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
